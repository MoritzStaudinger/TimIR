{"docno": "2019.sigirconf_workshop-2019birndl.0", "text": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019 ", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.0", "abstract": "", "title": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019", "authors": [], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.1", "text": "Preface: 4th Joint Workshop on BIRNDL at SIGIR 2019 ", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.1", "abstract": "", "title": "Preface: 4th Joint Workshop on BIRNDL at SIGIR 2019", "authors": [], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.2", "text": "Personalized Feed/Query-formulation, Predictive Impact, and Ranking The Meta discovery system is designed to aid biomedical researchers in keeping up to date on the most recent and most impactful research publications and preprints via personalized feeds and search. The service generates feeds of recent papers that are specific and relevant to each user's scientific interests by leveraging state of the art embeddings and clustering techniques. Meta also calculates article-level inferred Eigenfactor\u00ae scores which are used to rank the papers. This paper discusses Meta's approach to query formulation and ranking to improve retrieval of recently published, and yet un-cited academic publications.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.2", "abstract": "The Meta discovery system is designed to aid biomedical researchers in keeping up to date on the most recent and most impactful research publications and preprints via personalized feeds and search. The service generates feeds of recent papers that are specific and relevant to each user's scientific interests by leveraging state of the art embeddings and clustering techniques. Meta also calculates article-level inferred Eigenfactor\u00ae scores which are used to rank the papers. This paper discusses Meta's approach to query formulation and ranking to improve retrieval of recently published, and yet un-cited academic publications.", "title": "Personalized Feed/Query-formulation, Predictive Impact, and Ranking", "authors": ["Alex D. Wade", "Ivana Williams"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.3", "text": "Discourse Processing for Text Analysis: Recent Successes, Current Challenges Computational discourse processing has come a long way in the 10 years since I spoke at ACL'2009 on Discourse: Early problems, current successes, future challenges. Much of this progress can be attributed to the vast amounts of textual data that have become available and to a concomitant weakening of theoretical commitments, so as to be able to use the data in information extraction, sentiment analysis, question answering, etc. Along with weakened commitments to the demands of particular theories, has been a greater willingness to consider what can be learned from textual data and from various forms of annotation, in English and in other languages as well. This paper briefly summarizes (1) changing assumptions about discourse structure; (2) recent work on lexico-syntactic grounding of low-level discourse structure and frameworks for higher-level discourse structure that recognize differences in genre; and (3) suggestions for addressing some of the challenges still facing us. For more detail, the reader is encouraged to go to the references themselves.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.3", "abstract": "Computational discourse processing has come a long way in the 10 years since I spoke at ACL'2009 on Discourse: Early problems, current successes, future challenges. Much of this progress can be attributed to the vast amounts of textual data that have become available and to a concomitant weakening of theoretical commitments, so as to be able to use the data in information extraction, sentiment analysis, question answering, etc. Along with weakened commitments to the demands of particular theories, has been a greater willingness to consider what can be learned from textual data and from various forms of annotation, in English and in other languages as well. This paper briefly summarizes (1) changing assumptions about discourse structure; (2) recent work on lexico-syntactic grounding of low-level discourse structure and frameworks for higher-level discourse structure that recognize differences in genre; and (3) suggestions for addressing some of the challenges still facing us. For more detail, the reader is encouraged to go to the references themselves.", "title": "Discourse Processing for Text Analysis: Recent Successes, Current Challenges", "authors": ["Bonnie Webber"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.4", "text": "Distant Supervision for Silver Label Generation of Software Mentions in Social Scientific Publications Many scientific investigations rely on software for a range of different tasks including statistical data analyses, data pre-processing and data presentation. The choice of software may have a great influence not only on the research process but also on the derived findings, e.g. when errors in the used software lead to incorrect computations or biases. In order to increase transparency of research and verifiability of findings, knowledge of the used software thus is crucial. However, explicit links between publications and used software are usually not available. In addition, software is, unlike literature, often not cited in a standardized way which makes the automatic generation of links difficult. While recent Named Entity Recognition (NER) approaches yield excellent results for a wide range of use-cases and tasks, they typically require large sets of annotated data which may be hard to acquire. In this paper, we investigate the use of weakly supervised approaches with distant supervision to create silver labels to train supervised software mention extraction methods using transfer learning. We show that by combining even only a small number of weakly supervised approaches, a silver standard corpus can be created that serves as a useful basis for transfer learning.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.4", "abstract": "Many scientific investigations rely on software for a range of different tasks including statistical data analyses, data pre-processing and data presentation. The choice of software may have a great influence not only on the research process but also on the derived findings, e.g. when errors in the used software lead to incorrect computations or biases. In order to increase transparency of research and verifiability of findings, knowledge of the used software thus is crucial. However, explicit links between publications and used software are usually not available. In addition, software is, unlike literature, often not cited in a standardized way which makes the automatic generation of links difficult. While recent Named Entity Recognition (NER) approaches yield excellent results for a wide range of use-cases and tasks, they typically require large sets of annotated data which may be hard to acquire. In this paper, we investigate the use of weakly supervised approaches with distant supervision to create silver labels to train supervised software mention extraction methods using transfer learning. We show that by combining even only a small number of weakly supervised approaches, a silver standard corpus can be created that serves as a useful basis for transfer learning.", "title": "Distant Supervision for Silver Label Generation of Software Mentions in Social Scientific Publications", "authors": ["Katarina Boland", "Frank Kr\u00fcger"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.5", "text": "Transfer Learning for Scientific Data Chain Extraction in Small Chemical Corpus with joint BERT-CRF Model ", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.5", "abstract": "", "title": "Transfer Learning for Scientific Data Chain Extraction in Small Chemical Corpus with joint BERT-CRF Model", "authors": ["Na Pang", "Li Qian", "Weimin Lyu", "Jin-Dong Yang"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.6", "text": "Revaluating Semantometrics from Computer Science Publications The identification of important publications is subject in many research projects. While the influence of citations in finding seminal papers has been analysed thoroughly, semantic features of citation networks are regarded with less vigour. In this paper, we revaluate the ideas of semantometrics presented by Herrmannova et al.[9,13] to learn patterns of features extracted from publication distances in their citation networks aiming at distinguishing between seminal and survey papers in the area of computer science. For the evaluation, we present the SeminalSurveyDBLP dataset. By using different document content representations, the incorporation of semantic distance measures, as well as multiple machine learning algorithms for the classification, we achieved an accuracy of up to 0.8015 on our dataset. Earlier findings in this area suggest features extracted from references to be more suitable proxies whereas we observed the contrasting importance of features describing citation information.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.6", "abstract": "The identification of important publications is subject in many research projects. While the influence of citations in finding seminal papers has been analysed thoroughly, semantic features of citation networks are regarded with less vigour. In this paper, we revaluate the ideas of semantometrics presented by Herrmannova et al.[9,13] to learn patterns of features extracted from publication distances in their citation networks aiming at distinguishing between seminal and survey papers in the area of computer science. For the evaluation, we present the SeminalSurveyDBLP dataset. By using different document content representations, the incorporation of semantic distance measures, as well as multiple machine learning algorithms for the classification, we achieved an accuracy of up to 0.8015 on our dataset. Earlier findings in this area suggest features extracted from references to be more suitable proxies whereas we observed the contrasting importance of features describing citation information.", "title": "Revaluating Semantometrics from Computer Science Publications", "authors": ["Christin Katharina Kreutz", "Premtim Sahitaj", "Ralf Schenkel"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.7", "text": "Extracting and Matching Patent In-text References to Scientific Publications References in patent texts to scientific publications are valuable for studying the links between science and technology but are difficult to extract. This paper tackles this challenge, specifically, we extract references embedded in USPTO patent full texts and match them to Web of Science (WoS) publications. We approach the reference extraction problem as a sequence labelling task, training CRF and Flair models. We then match references to the WoS using regular expression patterns. We train and evaluate the reference extraction models using cross validation on a sample of 22 patents with 1,952 manually annotated in-text references. Then we apply the models to a large collection of 33,338 biotech patents. We find that CRF obtains better results on citation extraction than Flair, with precision scores of around 90% and recall of around 85%. However, Flair extracts much more references from the large collection than CRF, and more of those can be matched to WoS publications. We find that 88% of the extracted in-text references are not listed on patent front page, suggesting distinct roles played by in-text and front-page references. CRF and Flair collectively extract 603,457 references to WoS publications that are not listed on the front page. In addition to the 1.17 Million front-page references in the collection, this is a 51% increase in identified patent-publication links compared with only relying on frontpage references.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.7", "abstract": "References in patent texts to scientific publications are valuable for studying the links between science and technology but are difficult to extract. This paper tackles this challenge, specifically, we extract references embedded in USPTO patent full texts and match them to Web of Science (WoS) publications. We approach the reference extraction problem as a sequence labelling task, training CRF and Flair models. We then match references to the WoS using regular expression patterns. We train and evaluate the reference extraction models using cross validation on a sample of 22 patents with 1,952 manually annotated in-text references. Then we apply the models to a large collection of 33,338 biotech patents. We find that CRF obtains better results on citation extraction than Flair, with precision scores of around 90% and recall of around 85%. However, Flair extracts much more references from the large collection than CRF, and more of those can be matched to WoS publications. We find that 88% of the extracted in-text references are not listed on patent front page, suggesting distinct roles played by in-text and front-page references. CRF and Flair collectively extract 603,457 references to WoS publications that are not listed on the front page. In addition to the 1.17 Million front-page references in the collection, this is a 51% increase in identified patent-publication links compared with only relying on frontpage references.", "title": "Extracting and Matching Patent In-text References to Scientific Publications", "authors": ["Suzan Verberne", "Ioannis Chios", "Jian Wang"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.8", "text": "HITS Hits Readersourcing: Validating Peer Review Alternatives Using Network Analysis ", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.8", "abstract": "", "title": "HITS Hits Readersourcing: Validating Peer Review Alternatives Using Network Analysis", "authors": ["Michael Soprano", "Kevin Roitero", "Stefano Mizzaro"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.9", "text": "Supervised Learning for Automated Literature Review Automated methods to collect papers for literature reviews have the potential to save time and provide new insights. However, a lack of labeled ground-truth data has made it difficult to develop and evaluate these methods. We propose a framework to use the reference lists from existing review papers as labeled data to train supervised classifiers, allowing for experimentation and testing of models and features at a large scale. We demonstrate our method by training classifiers using both citation-and text-based features on 654 review papers. We also demonstrate how this method may be extended to generate a novel review collection for a newly emerging research field.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.9", "abstract": "Automated methods to collect papers for literature reviews have the potential to save time and provide new insights. However, a lack of labeled ground-truth data has made it difficult to develop and evaluate these methods. We propose a framework to use the reference lists from existing review papers as labeled data to train supervised classifiers, allowing for experimentation and testing of models and features at a large scale. We demonstrate our method by training classifiers using both citation-and text-based features on 654 review papers. We also demonstrate how this method may be extended to generate a novel review collection for a newly emerging research field.", "title": "Supervised Learning for Automated Literature Review", "authors": ["Jason Portenoy", "Jevin D. West"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.11", "text": "Comparing Word Embeddings for Document Screening based on Active Learning Document screening is a fundamental task within Evidencebased Medicine (EBM), a practice that provides scientific evidence to support medical decisions. Several approaches are attempting to reduce the workload of physicians who need to screen and label hundreds or thousands of documents in order to answer specific clinical questions. Previous works have attempted to semi-automate document screening, reporting promising results, but their evaluation is conducted using small datasets, which hinders generalization. Moreover, some recent works have used recently introduced neural language models, but no previous work have compared, for this task, the performance of different language models based on neural word embeddings, which have reported good results in the latest years for several NLP tasks. In this work, we evaluate the performance of two popular neural word embeddings (Word2vec and GloVe) in an active learning-based setting for document screening in EBM, with the goal of reducing the number of documents that physicians need to label in order to answer clinical questions. We evaluate these methods in a small public dataset (HealthCLEF 2017) as well as a larger one (Epistemonikos). Our experiments indicate that Word2vec have less variance and better general performance than GloVe when using active learning strategies based on uncertainty sampling.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.11", "abstract": "Document screening is a fundamental task within Evidencebased Medicine (EBM), a practice that provides scientific evidence to support medical decisions. Several approaches are attempting to reduce the workload of physicians who need to screen and label hundreds or thousands of documents in order to answer specific clinical questions. Previous works have attempted to semi-automate document screening, reporting promising results, but their evaluation is conducted using small datasets, which hinders generalization. Moreover, some recent works have used recently introduced neural language models, but no previous work have compared, for this task, the performance of different language models based on neural word embeddings, which have reported good results in the latest years for several NLP tasks. In this work, we evaluate the performance of two popular neural word embeddings (Word2vec and GloVe) in an active learning-based setting for document screening in EBM, with the goal of reducing the number of documents that physicians need to label in order to answer clinical questions. We evaluate these methods in a small public dataset (HealthCLEF 2017) as well as a larger one (Epistemonikos). Our experiments indicate that Word2vec have less variance and better general performance than GloVe when using active learning strategies based on uncertainty sampling.", "title": "Comparing Word Embeddings for Document Screening based on Active Learning", "authors": ["Andres Carvallo", "Denis Parra"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
{"docno": "2019.sigirconf_workshop-2019birndl.10", "text": "Can Models of Author Intention Support Quality Assessment of Content? Academics seek to find, understand and critically review the work of other researchers through published scientific articles. In recent years, the volume of available information has significantly increased, partly due to technological advancements and partly due to pressures on academics to 'publish or perish'. This amount of papers presents a challenge not only for the peer-review process but also for readers, particularly inexperienced readers, to find publications of high quality. Whilst one might rely on citation or journal rankings to help guide this decision, this approach may not be completely reliable due to biased peer-review processes and the fact that the citation count of an article does not per se indicate its quality. Here, we analyse how expected author intentions in a Related Work section can be used to indicate its quality. We show that author intentions can predict the quality with reasonable accuracy and propose that similar approaches could be used in other sections to provide an overall picture of quality. This approach could be useful in supporting peer-review processes and for a reader in prioritising articles to read.", "original_document": {"doc_id": "2019.sigirconf_workshop-2019birndl.10", "abstract": "Academics seek to find, understand and critically review the work of other researchers through published scientific articles. In recent years, the volume of available information has significantly increased, partly due to technological advancements and partly due to pressures on academics to 'publish or perish'. This amount of papers presents a challenge not only for the peer-review process but also for readers, particularly inexperienced readers, to find publications of high quality. Whilst one might rely on citation or journal rankings to help guide this decision, this approach may not be completely reliable due to biased peer-review processes and the fact that the citation count of an article does not per se indicate its quality. Here, we analyse how expected author intentions in a Related Work section can be used to indicate its quality. We show that author intentions can predict the quality with reasonable accuracy and propose that similar approaches could be used in other sections to provide an overall picture of quality. This approach could be useful in supporting peer-review processes and for a reader in prioritising articles to read.", "title": "Can Models of Author Intention Support Quality Assessment of Content?", "authors": ["Arlene J. Casey", "Bonnie Webber", "Dorota Glowacka"], "year": "2019", "booktitle": "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019"}}
